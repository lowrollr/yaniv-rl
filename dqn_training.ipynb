{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rlcard\n",
    "from rlcard.envs.registration import register\n",
    "from rlcard.utils import get_device, Logger, reorganize, plot_curve\n",
    "from psuedo_optimal import PsuedoOptimalAgent\n",
    "\n",
    "from rlcard.agents import DQNAgent\n",
    "import torch\n",
    "import numpy as np\n",
    "import argparse\n",
    "import os\n",
    "from copy import deepcopy\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "register(\n",
    "    env_id='yaniv',\n",
    "    entry_point='env:YanivEnv',\n",
    ")\n",
    "rlcard.make('yaniv', config={'game_num_players': 4})\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = rlcard.make('yaniv')\n",
    "eval_env = rlcard.make('yaniv')\n",
    "\n",
    "agents = [DQNAgent(\n",
    "                num_actions=env.num_actions,\n",
    "                state_shape=env.state_shape[0],\n",
    "                mlp_layers=[64, 64],\n",
    "                device=device\n",
    "            )\n",
    "        ] + [PsuedoOptimalAgent(num_actions=env.num_actions) for _ in range(env.num_players - 1)]\n",
    "\n",
    "env.set_agents(agents)\n",
    "\n",
    "eval_agents = [agents[0]] + [PsuedoOptimalAgent(num_actions=eval_env.num_actions) for _ in range(eval_env.num_players - 1)]\n",
    "eval_env.set_agents(eval_agents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPISODES = 1000\n",
    "EVAL_EVERY = 100\n",
    "TOP_MODEL_TOURNAMENT_GAMES = 3\n",
    "BASELINE_EVAL_GAMES = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tournament(env, num):\n",
    "    payoffs = [0 for _ in range(env.num_players)]\n",
    "    running_turns = 0\n",
    "    counter = 0\n",
    "    \n",
    "    while counter < num:\n",
    "        _, _payoffs = env.run(is_training=False)\n",
    "        running_turns += len(env.action_recorder) / env.num_players\n",
    "        if isinstance(_payoffs, list):\n",
    "            for _p in _payoffs:\n",
    "                for i, _ in enumerate(payoffs):\n",
    "                    payoffs[i] += _p[i]\n",
    "                counter += 1\n",
    "        else:\n",
    "            for i, _ in enumerate(payoffs):\n",
    "                payoffs[i] += _payoffs[i]\n",
    "            counter += 1\n",
    "    for i, _ in enumerate(payoffs):\n",
    "        payoffs[i] /= counter\n",
    "    return payoffs, running_turns / counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/marshingjay/mambaforge/envs/python38/lib/python3.8/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - Step 100, rl-loss: 0.13697469234466553\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 1100, rl-loss: 33.7894973754882874\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 1664, rl-loss: 8.09281158447265645"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [6], line 17\u001b[0m\n\u001b[1;32m     11\u001b[0m     env\u001b[39m.\u001b[39magents[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mfeed(ts)\n\u001b[1;32m     14\u001b[0m \u001b[39mif\u001b[39;00m episode \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m episode \u001b[39m%\u001b[39m EVAL_EVERY \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m     15\u001b[0m     \u001b[39m# grab best agent \u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m     payoffs, mean_turns \u001b[39m=\u001b[39m tournament(eval_env, BASELINE_EVAL_GAMES)\n\u001b[1;32m     18\u001b[0m     baseline_scores\u001b[39m.\u001b[39mappend(payoffs[\u001b[39m0\u001b[39m])\n\u001b[1;32m     19\u001b[0m     baseline_turns\u001b[39m.\u001b[39mappend(mean_turns)\n",
      "Cell \u001b[0;32mIn [5], line 7\u001b[0m, in \u001b[0;36mtournament\u001b[0;34m(env, num)\u001b[0m\n\u001b[1;32m      4\u001b[0m counter \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[39mwhile\u001b[39;00m counter \u001b[39m<\u001b[39m num:\n\u001b[0;32m----> 7\u001b[0m     _, _payoffs \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mrun(is_training\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m      8\u001b[0m     running_turns \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(env\u001b[39m.\u001b[39maction_recorder) \u001b[39m/\u001b[39m env\u001b[39m.\u001b[39mnum_players\n\u001b[1;32m      9\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(_payoffs, \u001b[39mlist\u001b[39m):\n",
      "File \u001b[0;32m~/mambaforge/envs/python38/lib/python3.8/site-packages/rlcard/envs/env.py:144\u001b[0m, in \u001b[0;36mEnv.run\u001b[0;34m(self, is_training)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_over():\n\u001b[1;32m    142\u001b[0m     \u001b[39m# Agent plays\u001b[39;00m\n\u001b[1;32m    143\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_training:\n\u001b[0;32m--> 144\u001b[0m         action, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49magents[player_id]\u001b[39m.\u001b[39;49meval_step(state)\n\u001b[1;32m    145\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    146\u001b[0m         action \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39magents[player_id]\u001b[39m.\u001b[39mstep(state)\n",
      "File \u001b[0;32m~/Repos/yaniv-rl/psuedo_optimal.py:62\u001b[0m, in \u001b[0;36mPsuedoOptimalAgent.eval_step\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     58\u001b[0m         scores\u001b[39m.\u001b[39mappend((most_played, \u001b[39m-\u001b[39mnew_hand_value, action))\n\u001b[1;32m     60\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mmax\u001b[39m(scores)[\u001b[39m2\u001b[39m]\u001b[39m.\u001b[39m\u001b[39m__hash__\u001b[39m()\n\u001b[0;32m---> 62\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39meval_step\u001b[39m(\u001b[39mself\u001b[39m, state):\n\u001b[1;32m     63\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstep(state), {}\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "baseline_scores = []\n",
    "baseline_turns = []\n",
    "top_tournament_scores = []\n",
    "top_tournament_turns = []\n",
    "\n",
    "\n",
    "for episode in range(NUM_EPISODES):\n",
    "    trajectories, payoffs = env.run(is_training=True)\n",
    "    trajectories = reorganize(trajectories, payoffs)\n",
    "    for ts in trajectories[0]:\n",
    "        env.agents[0].feed(ts)\n",
    "    \n",
    "    \n",
    "    if episode != 0 and episode % EVAL_EVERY == 0:\n",
    "        # grab best agent \n",
    "        \n",
    "        payoffs, mean_turns = tournament(eval_env, BASELINE_EVAL_GAMES)\n",
    "        baseline_scores.append(payoffs[0])\n",
    "        baseline_turns.append(mean_turns)\n",
    "\n",
    "        print(f'episode {episode} Top Model vs. Random Mean Score: {baseline_scores[-1]}')\n",
    "        print(f'episode {episode} Top Model vs. Random Mean Turns: {baseline_turns[-1]}')\n",
    "        print(f'episode {episode} Models vs. Models Mean Score: {top_tournament_scores[-1]}')\n",
    "        print(f'episode {episode} Models vs. Models Mean Turns: {top_tournament_turns[-1]}')\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = './'\n",
    "save_path = os.path.join(save_dir, f'model_gen.pth')\n",
    "torch.save(agent, save_path)\n",
    "print('Model saved in', save_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13 | packaged by conda-forge | (default, Mar 25 2022, 06:05:16) \n[Clang 12.0.1 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "714499164d6db2eee6aed234274a1e2b8b6b7663ab8cb01ef38bcdcb5f9c772e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
