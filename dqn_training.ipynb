{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rlcard\n",
    "from rlcard.envs.registration import register\n",
    "from rlcard.utils import get_device, Logger, reorganize, plot_curve\n",
    "from rlcard.agents import RandomAgent\n",
    "\n",
    "from rlcard.agents import DQNAgent\n",
    "import torch\n",
    "import numpy as np\n",
    "import argparse\n",
    "import os\n",
    "from copy import deepcopy\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "register(\n",
    "    env_id='yaniv',\n",
    "    entry_point='env:YanivEnv',\n",
    ")\n",
    "rlcard.make('yaniv', config={'game_num_players': 4})\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = rlcard.make('yaniv')\n",
    "eval_env = rlcard.make('yaniv')\n",
    "\n",
    "agents = [DQNAgent(\n",
    "                num_actions=env.num_actions,\n",
    "                state_shape=env.state_shape[0],\n",
    "                mlp_layers=[64, 64],\n",
    "                device=device\n",
    "            )\n",
    "            for _ in range(env.num_players)]\n",
    "\n",
    "env.set_agents(agents)\n",
    "\n",
    "eval_agents = [agents[0]] + [RandomAgent(num_actions=eval_env.num_actions) for _ in range(eval_env.num_players - 1)]\n",
    "eval_env.set_agents(eval_agents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPISODES = 1000\n",
    "EVAL_EVERY = 100\n",
    "TOP_MODEL_TOURNAMENT_GAMES = 3\n",
    "BASELINE_EVAL_GAMES = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tournament(env, num):\n",
    "    payoffs = [0 for _ in range(env.num_players)]\n",
    "    running_turns = 0\n",
    "    counter = 0\n",
    "    \n",
    "    while counter < num:\n",
    "        _, _payoffs = env.run(is_training=False)\n",
    "        running_turns += len(env.action_recorder) / env.num_players\n",
    "        if isinstance(_payoffs, list):\n",
    "            for _p in _payoffs:\n",
    "                for i, _ in enumerate(payoffs):\n",
    "                    payoffs[i] += _p[i]\n",
    "                counter += 1\n",
    "        else:\n",
    "            for i, _ in enumerate(payoffs):\n",
    "                payoffs[i] += _payoffs[i]\n",
    "            counter += 1\n",
    "    for i, _ in enumerate(payoffs):\n",
    "        payoffs[i] /= counter\n",
    "    return payoffs, running_turns / counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/marshingjay/mambaforge/envs/python38/lib/python3.8/site-packages/rlcard/agents/dqn_agent.py:194: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  state_batch, action_batch, reward_batch, next_state_batch, legal_actions_batch, done_batch = self.memory.sample()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - Step 44101, rl-loss: 0.65396791696548466\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 44100, rl-loss: 2.0162034034729004\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 44101, rl-loss: 4.7664752006530766\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 44101, rl-loss: 1.3373063802719116\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 45101, rl-loss: 3.70091176033023273\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 45100, rl-loss: 15.256819725036621\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 45101, rl-loss: 0.6305344104766846\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 45101, rl-loss: 1.3145532608032227\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 46101, rl-loss: 1.36679351329803477\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 46100, rl-loss: 3.428567409515381\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 46101, rl-loss: 1.3908288478851318\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 46101, rl-loss: 1.0216826200485234\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 47101, rl-loss: 1.45706462860107426\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 47100, rl-loss: 4.183554172515869\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 47101, rl-loss: 3.00283575057983466\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 47101, rl-loss: 1.2606024742126465\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 47488, rl-loss: 0.45149916410446167episode 100 Top Model vs. Random Mean Score: -13.333333333333334\n",
      "episode 100 Top Model vs. Random Mean Turns: 58.25\n",
      "episode 100 Models vs. Models Mean Score: -13.666666666666666\n",
      "episode 100 Models vs. Models Mean Turns: 40.666666666666664\n",
      "INFO - Step 48101, rl-loss: 0.65513008832931523\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 48100, rl-loss: 0.909565806388855\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 48101, rl-loss: 13.500077247619629\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 48101, rl-loss: 0.49225613474845886\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 49101, rl-loss: 16.3044452667236335\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 49100, rl-loss: 4.005690574645996\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 49101, rl-loss: 1.8269358873367313\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 49101, rl-loss: 6.843966007232666\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 50101, rl-loss: 3.51540660858154372\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 50100, rl-loss: 1.1490638256072998\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 50101, rl-loss: 0.6446859836578369\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 50101, rl-loss: 1.7515479326248176\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 51101, rl-loss: 1.75526118278503427\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 51100, rl-loss: 1.3945386409759521\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 51101, rl-loss: 0.9855623245239258\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 51101, rl-loss: 2.5921680927276612\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 52069, rl-loss: 6.40145349502563545episode 200 Top Model vs. Random Mean Score: -14.333333333333334\n",
      "episode 200 Top Model vs. Random Mean Turns: 50.0\n",
      "episode 200 Models vs. Models Mean Score: -13.166666666666666\n",
      "episode 200 Models vs. Models Mean Turns: 59.916666666666664\n",
      "INFO - Step 52101, rl-loss: 0.8506769537925721\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 52100, rl-loss: 3.443194627761841\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 52101, rl-loss: 2.7020468711853027\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 52101, rl-loss: 1.1185306310653687\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 53101, rl-loss: 1.54191613197326665\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 53100, rl-loss: 2.678497076034546\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 53101, rl-loss: 11.982116699218752\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 53101, rl-loss: 1.5645509958267212\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 54101, rl-loss: 2.45625281333923343\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 54100, rl-loss: 2.294119358062744\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 54101, rl-loss: 1.7969090938568115\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 54101, rl-loss: 1.7599291801452637\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 55101, rl-loss: 26.3896141052246143\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 55100, rl-loss: 1.5970516204833984\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 55101, rl-loss: 18.390556335449225\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 55101, rl-loss: 3.4857230186462402\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 56101, rl-loss: 2.09798049926757825\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 56100, rl-loss: 1.1535905599594116\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 56101, rl-loss: 3.0400335788726807\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 56101, rl-loss: 1.2318111658096313\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 57101, rl-loss: 0.88847690820693976\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 57100, rl-loss: 1.0105136632919312\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 57101, rl-loss: 1.9307552576065063\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 57101, rl-loss: 19.591293334960938\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 57367, rl-loss: 3.41084384918212926"
     ]
    }
   ],
   "source": [
    "baseline_scores = []\n",
    "baseline_turns = []\n",
    "top_tournament_scores = []\n",
    "top_tournament_turns = []\n",
    "\n",
    "\n",
    "for episode in range(NUM_EPISODES):\n",
    "    trajectories, payoffs = env.run(is_training=True)\n",
    "    trajectories = reorganize(trajectories, payoffs)\n",
    "    for ts in trajectories[0]:\n",
    "        for agent in agents:\n",
    "            agent.feed(ts)\n",
    "    \n",
    "    \n",
    "    if episode != 0 and episode % EVAL_EVERY == 0:\n",
    "        # grab best agent \n",
    "        payoffs, mean_turns = tournament(env, TOP_MODEL_TOURNAMENT_GAMES)\n",
    "        top_tournament_scores.append(sum(payoffs) / env.num_players)\n",
    "        top_tournament_turns.append(mean_turns)\n",
    "        best = np.argmax(payoffs)\n",
    "        env.set_agents([agents[best]] + [deepcopy(agents[best]) for _ in range(env.num_players - 1)])\n",
    "        \n",
    "        payoffs, mean_turns = tournament(eval_env, BASELINE_EVAL_GAMES)\n",
    "        baseline_scores.append(payoffs[0])\n",
    "        baseline_turns.append(mean_turns)\n",
    "\n",
    "        print(f'episode {episode} Top Model vs. Random Mean Score: {baseline_scores[-1]}')\n",
    "        print(f'episode {episode} Top Model vs. Random Mean Turns: {baseline_turns[-1]}')\n",
    "        print(f'episode {episode} Models vs. Models Mean Score: {top_tournament_scores[-1]}')\n",
    "        print(f'episode {episode} Models vs. Models Mean Turns: {top_tournament_turns[-1]}')\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = './'\n",
    "save_path = os.path.join(save_dir, f'model_gen.pth')\n",
    "torch.save(agent, save_path)\n",
    "print('Model saved in', save_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "714499164d6db2eee6aed234274a1e2b8b6b7663ab8cb01ef38bcdcb5f9c772e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
